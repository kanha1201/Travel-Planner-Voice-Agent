# Voice Travel Planner - Environment Variables
# Copy this file to .env in the backend directory and fill in your API keys

# LLM Provider Configuration
# Set to "cerebras", "groq", or "gemini" (default: "cerebras")
# Fallback chain: Cerebras -> Groq -> Gemini (automatic)
LLM_PROVIDER=cerebras

# Cerebras API Configuration (Primary Provider)
CEREBRAS_API_KEY=your_cerebras_api_key_here
CEREBRAS_BASE_URL=https://api.cerebras.ai/v1
CEREBRAS_MODEL=llama-3.3-70b

# Groq API Configuration (Fallback Provider)
GROQ_API_KEY=your_groq_api_key_here

# Gemini API Configuration (Last Resort Fallback)
GEMINI_API_KEY=your_gemini_api_key_here

# Speech-to-Text (STT) Configuration
# Provider: "assemblyai" (recommended) or "elevenlabs"
# Note: AssemblyAI requires ffmpeg for WebM conversion (install from https://ffmpeg.org/download.html)
STT_PROVIDER=assemblyai

# ElevenLabs STT Configuration
ELEVENLABS_API_KEY=your_elevenlabs_api_key_here
ELEVENLABS_BASE_URL=https://api.elevenlabs.io/v1
ELEVENLABS_STT_MODEL_ID=scribe_v1
# Optional: If your ElevenLabs STT uses a different endpoint path
# ELEVENLABS_STT_ENDPOINT=speech-to-text  # or "transcribe", "stt", etc.

# AssemblyAI STT (Alternative - if you want to use AssemblyAI for STT)
ASSEMBLYAI_API_KEY=your_assemblyai_api_key_here
ASSEMBLYAI_BASE_URL=https://api.assemblyai.com/v2

# Text-to-Speech (TTS) Configuration
# Provider: "elevenlabs" (recommended) or "assemblyai"
# Note: AssemblyAI does NOT offer TTS. Use ElevenLabs for TTS.
TTS_PROVIDER=elevenlabs

# ElevenLabs TTS Configuration
ELEVENLABS_VOICE_ID=21m00Tcm4TlvDq8ikWAM

# Application Configuration
ENVIRONMENT=development  # development, production
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR

# Server Configuration
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000

# CORS Configuration
CORS_ORIGINS=http://localhost:3000,http://localhost:5173  # Add your frontend URLs

# Session Configuration
SESSION_TIMEOUT_MINUTES=30

# Cache Configuration
POI_CACHE_TTL_DAYS=7

# Retry Configuration
MAX_RETRY_ATTEMPTS=3
MAX_RETRY_TIMEOUT_SECONDS=45

# Cache Configuration (Optimization to reduce API calls)
# Response cache for LLM responses (reduces API calls)
RESPONSE_CACHE_TTL_MINUTES=60
RESPONSE_CACHE_MAX_SIZE=1000

# Tool cache for function results (reduces redundant tool calls)
TOOL_CACHE_TTL_HOURS=24
TOOL_CACHE_MAX_SIZE=500






